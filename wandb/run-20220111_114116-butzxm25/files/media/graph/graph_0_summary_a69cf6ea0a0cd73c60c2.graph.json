{"format": "torch", "nodes": [{"name": "transformer", "id": 139643632200576, "class_name": "Transf(\n  (transformer): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (4): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (5): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (6): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (7): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (8): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (9): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (10): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (11): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (12): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (13): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (14): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (15): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (16): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (17): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (18): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (19): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n        )\n        (linear1): Linear(in_features=5, out_features=20, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=20, out_features=5, bias=True)\n        (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (fc1): Linear(in_features=20, out_features=5, bias=True)\n  (fc2): Linear(in_features=5, out_features=1, bias=True)\n)", "parameters": [["transformer.layers.0.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.0.self_attn.in_proj_bias", [15]], ["transformer.layers.0.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.0.self_attn.out_proj.bias", [5]], ["transformer.layers.0.linear1.weight", [20, 5]], ["transformer.layers.0.linear1.bias", [20]], ["transformer.layers.0.linear2.weight", [5, 20]], ["transformer.layers.0.linear2.bias", [5]], ["transformer.layers.0.norm1.weight", [5]], ["transformer.layers.0.norm1.bias", [5]], ["transformer.layers.0.norm2.weight", [5]], ["transformer.layers.0.norm2.bias", [5]], ["transformer.layers.1.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.1.self_attn.in_proj_bias", [15]], ["transformer.layers.1.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.1.self_attn.out_proj.bias", [5]], ["transformer.layers.1.linear1.weight", [20, 5]], ["transformer.layers.1.linear1.bias", [20]], ["transformer.layers.1.linear2.weight", [5, 20]], ["transformer.layers.1.linear2.bias", [5]], ["transformer.layers.1.norm1.weight", [5]], ["transformer.layers.1.norm1.bias", [5]], ["transformer.layers.1.norm2.weight", [5]], ["transformer.layers.1.norm2.bias", [5]], ["transformer.layers.2.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.2.self_attn.in_proj_bias", [15]], ["transformer.layers.2.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.2.self_attn.out_proj.bias", [5]], ["transformer.layers.2.linear1.weight", [20, 5]], ["transformer.layers.2.linear1.bias", [20]], ["transformer.layers.2.linear2.weight", [5, 20]], ["transformer.layers.2.linear2.bias", [5]], ["transformer.layers.2.norm1.weight", [5]], ["transformer.layers.2.norm1.bias", [5]], ["transformer.layers.2.norm2.weight", [5]], ["transformer.layers.2.norm2.bias", [5]], ["transformer.layers.3.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.3.self_attn.in_proj_bias", [15]], ["transformer.layers.3.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.3.self_attn.out_proj.bias", [5]], ["transformer.layers.3.linear1.weight", [20, 5]], ["transformer.layers.3.linear1.bias", [20]], ["transformer.layers.3.linear2.weight", [5, 20]], ["transformer.layers.3.linear2.bias", [5]], ["transformer.layers.3.norm1.weight", [5]], ["transformer.layers.3.norm1.bias", [5]], ["transformer.layers.3.norm2.weight", [5]], ["transformer.layers.3.norm2.bias", [5]], ["transformer.layers.4.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.4.self_attn.in_proj_bias", [15]], ["transformer.layers.4.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.4.self_attn.out_proj.bias", [5]], ["transformer.layers.4.linear1.weight", [20, 5]], ["transformer.layers.4.linear1.bias", [20]], ["transformer.layers.4.linear2.weight", [5, 20]], ["transformer.layers.4.linear2.bias", [5]], ["transformer.layers.4.norm1.weight", [5]], ["transformer.layers.4.norm1.bias", [5]], ["transformer.layers.4.norm2.weight", [5]], ["transformer.layers.4.norm2.bias", [5]], ["transformer.layers.5.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.5.self_attn.in_proj_bias", [15]], ["transformer.layers.5.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.5.self_attn.out_proj.bias", [5]], ["transformer.layers.5.linear1.weight", [20, 5]], ["transformer.layers.5.linear1.bias", [20]], ["transformer.layers.5.linear2.weight", [5, 20]], ["transformer.layers.5.linear2.bias", [5]], ["transformer.layers.5.norm1.weight", [5]], ["transformer.layers.5.norm1.bias", [5]], ["transformer.layers.5.norm2.weight", [5]], ["transformer.layers.5.norm2.bias", [5]], ["transformer.layers.6.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.6.self_attn.in_proj_bias", [15]], ["transformer.layers.6.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.6.self_attn.out_proj.bias", [5]], ["transformer.layers.6.linear1.weight", [20, 5]], ["transformer.layers.6.linear1.bias", [20]], ["transformer.layers.6.linear2.weight", [5, 20]], ["transformer.layers.6.linear2.bias", [5]], ["transformer.layers.6.norm1.weight", [5]], ["transformer.layers.6.norm1.bias", [5]], ["transformer.layers.6.norm2.weight", [5]], ["transformer.layers.6.norm2.bias", [5]], ["transformer.layers.7.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.7.self_attn.in_proj_bias", [15]], ["transformer.layers.7.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.7.self_attn.out_proj.bias", [5]], ["transformer.layers.7.linear1.weight", [20, 5]], ["transformer.layers.7.linear1.bias", [20]], ["transformer.layers.7.linear2.weight", [5, 20]], ["transformer.layers.7.linear2.bias", [5]], ["transformer.layers.7.norm1.weight", [5]], ["transformer.layers.7.norm1.bias", [5]], ["transformer.layers.7.norm2.weight", [5]], ["transformer.layers.7.norm2.bias", [5]], ["transformer.layers.8.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.8.self_attn.in_proj_bias", [15]], ["transformer.layers.8.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.8.self_attn.out_proj.bias", [5]], ["transformer.layers.8.linear1.weight", [20, 5]], ["transformer.layers.8.linear1.bias", [20]], ["transformer.layers.8.linear2.weight", [5, 20]], ["transformer.layers.8.linear2.bias", [5]], ["transformer.layers.8.norm1.weight", [5]], ["transformer.layers.8.norm1.bias", [5]], ["transformer.layers.8.norm2.weight", [5]], ["transformer.layers.8.norm2.bias", [5]], ["transformer.layers.9.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.9.self_attn.in_proj_bias", [15]], ["transformer.layers.9.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.9.self_attn.out_proj.bias", [5]], ["transformer.layers.9.linear1.weight", [20, 5]], ["transformer.layers.9.linear1.bias", [20]], ["transformer.layers.9.linear2.weight", [5, 20]], ["transformer.layers.9.linear2.bias", [5]], ["transformer.layers.9.norm1.weight", [5]], ["transformer.layers.9.norm1.bias", [5]], ["transformer.layers.9.norm2.weight", [5]], ["transformer.layers.9.norm2.bias", [5]], ["transformer.layers.10.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.10.self_attn.in_proj_bias", [15]], ["transformer.layers.10.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.10.self_attn.out_proj.bias", [5]], ["transformer.layers.10.linear1.weight", [20, 5]], ["transformer.layers.10.linear1.bias", [20]], ["transformer.layers.10.linear2.weight", [5, 20]], ["transformer.layers.10.linear2.bias", [5]], ["transformer.layers.10.norm1.weight", [5]], ["transformer.layers.10.norm1.bias", [5]], ["transformer.layers.10.norm2.weight", [5]], ["transformer.layers.10.norm2.bias", [5]], ["transformer.layers.11.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.11.self_attn.in_proj_bias", [15]], ["transformer.layers.11.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.11.self_attn.out_proj.bias", [5]], ["transformer.layers.11.linear1.weight", [20, 5]], ["transformer.layers.11.linear1.bias", [20]], ["transformer.layers.11.linear2.weight", [5, 20]], ["transformer.layers.11.linear2.bias", [5]], ["transformer.layers.11.norm1.weight", [5]], ["transformer.layers.11.norm1.bias", [5]], ["transformer.layers.11.norm2.weight", [5]], ["transformer.layers.11.norm2.bias", [5]], ["transformer.layers.12.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.12.self_attn.in_proj_bias", [15]], ["transformer.layers.12.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.12.self_attn.out_proj.bias", [5]], ["transformer.layers.12.linear1.weight", [20, 5]], ["transformer.layers.12.linear1.bias", [20]], ["transformer.layers.12.linear2.weight", [5, 20]], ["transformer.layers.12.linear2.bias", [5]], ["transformer.layers.12.norm1.weight", [5]], ["transformer.layers.12.norm1.bias", [5]], ["transformer.layers.12.norm2.weight", [5]], ["transformer.layers.12.norm2.bias", [5]], ["transformer.layers.13.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.13.self_attn.in_proj_bias", [15]], ["transformer.layers.13.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.13.self_attn.out_proj.bias", [5]], ["transformer.layers.13.linear1.weight", [20, 5]], ["transformer.layers.13.linear1.bias", [20]], ["transformer.layers.13.linear2.weight", [5, 20]], ["transformer.layers.13.linear2.bias", [5]], ["transformer.layers.13.norm1.weight", [5]], ["transformer.layers.13.norm1.bias", [5]], ["transformer.layers.13.norm2.weight", [5]], ["transformer.layers.13.norm2.bias", [5]], ["transformer.layers.14.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.14.self_attn.in_proj_bias", [15]], ["transformer.layers.14.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.14.self_attn.out_proj.bias", [5]], ["transformer.layers.14.linear1.weight", [20, 5]], ["transformer.layers.14.linear1.bias", [20]], ["transformer.layers.14.linear2.weight", [5, 20]], ["transformer.layers.14.linear2.bias", [5]], ["transformer.layers.14.norm1.weight", [5]], ["transformer.layers.14.norm1.bias", [5]], ["transformer.layers.14.norm2.weight", [5]], ["transformer.layers.14.norm2.bias", [5]], ["transformer.layers.15.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.15.self_attn.in_proj_bias", [15]], ["transformer.layers.15.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.15.self_attn.out_proj.bias", [5]], ["transformer.layers.15.linear1.weight", [20, 5]], ["transformer.layers.15.linear1.bias", [20]], ["transformer.layers.15.linear2.weight", [5, 20]], ["transformer.layers.15.linear2.bias", [5]], ["transformer.layers.15.norm1.weight", [5]], ["transformer.layers.15.norm1.bias", [5]], ["transformer.layers.15.norm2.weight", [5]], ["transformer.layers.15.norm2.bias", [5]], ["transformer.layers.16.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.16.self_attn.in_proj_bias", [15]], ["transformer.layers.16.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.16.self_attn.out_proj.bias", [5]], ["transformer.layers.16.linear1.weight", [20, 5]], ["transformer.layers.16.linear1.bias", [20]], ["transformer.layers.16.linear2.weight", [5, 20]], ["transformer.layers.16.linear2.bias", [5]], ["transformer.layers.16.norm1.weight", [5]], ["transformer.layers.16.norm1.bias", [5]], ["transformer.layers.16.norm2.weight", [5]], ["transformer.layers.16.norm2.bias", [5]], ["transformer.layers.17.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.17.self_attn.in_proj_bias", [15]], ["transformer.layers.17.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.17.self_attn.out_proj.bias", [5]], ["transformer.layers.17.linear1.weight", [20, 5]], ["transformer.layers.17.linear1.bias", [20]], ["transformer.layers.17.linear2.weight", [5, 20]], ["transformer.layers.17.linear2.bias", [5]], ["transformer.layers.17.norm1.weight", [5]], ["transformer.layers.17.norm1.bias", [5]], ["transformer.layers.17.norm2.weight", [5]], ["transformer.layers.17.norm2.bias", [5]], ["transformer.layers.18.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.18.self_attn.in_proj_bias", [15]], ["transformer.layers.18.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.18.self_attn.out_proj.bias", [5]], ["transformer.layers.18.linear1.weight", [20, 5]], ["transformer.layers.18.linear1.bias", [20]], ["transformer.layers.18.linear2.weight", [5, 20]], ["transformer.layers.18.linear2.bias", [5]], ["transformer.layers.18.norm1.weight", [5]], ["transformer.layers.18.norm1.bias", [5]], ["transformer.layers.18.norm2.weight", [5]], ["transformer.layers.18.norm2.bias", [5]], ["transformer.layers.19.self_attn.in_proj_weight", [15, 5]], ["transformer.layers.19.self_attn.in_proj_bias", [15]], ["transformer.layers.19.self_attn.out_proj.weight", [5, 5]], ["transformer.layers.19.self_attn.out_proj.bias", [5]], ["transformer.layers.19.linear1.weight", [20, 5]], ["transformer.layers.19.linear1.bias", [20]], ["transformer.layers.19.linear2.weight", [5, 20]], ["transformer.layers.19.linear2.bias", [5]], ["transformer.layers.19.norm1.weight", [5]], ["transformer.layers.19.norm1.bias", [5]], ["transformer.layers.19.norm2.weight", [5]], ["transformer.layers.19.norm2.bias", [5]], ["fc1.weight", [5, 20]], ["fc1.bias", [5]], ["fc2.weight", [1, 5]], ["fc2.bias", [1]]], "output_shape": [[16, 1]], "num_parameters": [75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 75, 15, 25, 5, 100, 20, 100, 5, 5, 5, 5, 5, 100, 5, 5, 1]}], "edges": []}